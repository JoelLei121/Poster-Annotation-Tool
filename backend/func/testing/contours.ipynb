{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_images_elem' from 'text_detector' (c:\\CodeForClass\\imgtool\\backend\\func\\text_detector.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_detector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_images_elem \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay\u001b[39m(mat, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_images_elem' from 'text_detector' (c:\\CodeForClass\\imgtool\\backend\\func\\text_detector.py)"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import io, random\n",
    "from PIL import Image\n",
    "from text_detector import get_image_elem \n",
    "\n",
    "def display(mat, name=None):\n",
    "    if name is None:\n",
    "        name = random.randint(1000, 9999)\n",
    "        name = str(int(name))\n",
    "    cv.namedWindow(name)\n",
    "    cv.imshow(name, mat)\n",
    "    return name\n",
    "    # cv.waitKey()\n",
    "    # try:\n",
    "    #     cv.destroyWindow('test')\n",
    "    # except:\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4390\n"
     ]
    }
   ],
   "source": [
    "TEST_IMG = 'test7.jpg'\n",
    "\n",
    "im = cv.imread(TEST_IMG)\n",
    "# im = cv.resize(im, (700, 960))\n",
    "\n",
    "edges = cv.Canny(im, 100, 200)\n",
    "cv.imwrite('test1_edge.jpg', edges)\n",
    "\n",
    "contours, _ = cv.findContours(edges, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "print(len(contours))\n",
    "\n",
    "new_im = cv.drawContours(im.copy(), contours, -1, (255,255,0), 2)\n",
    "\n",
    "# display(im, 'source')\n",
    "display(edges, 'edge')\n",
    "display(new_im, 'contours')\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\Joel\\.keras-ocr\\craft_mlt_25k.h5\n",
      "Looking for C:\\Users\\Joel\\.keras-ocr\\crnn_kurapan.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1500,6) (3,) (1500,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# im_b = Image.open(TEST_IMG)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# b = io.BytesIO()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# im_b.save(b, 'jpeg')\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m bboxs \u001b[38;5;241m=\u001b[39m \u001b[43mget_images_elem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(bboxs)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# display(im)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\CodeForClass\\imgtool\\backend\\func\\text_detector.py:61\u001b[0m, in \u001b[0;36mget_images_elem\u001b[1;34m(image_byte_list)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[0;32m     60\u001b[0m     init_pipeline()\n\u001b[1;32m---> 61\u001b[0m prediction_groups \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_byte_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m elem_groups \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m prediction_groups:\n",
      "File \u001b[1;32mc:\\CodeForClass\\imgtool\\backend\\venv\\lib\\site-packages\\keras_ocr\\pipeline.py:62\u001b[0m, in \u001b[0;36mPipeline.recognize\u001b[1;34m(self, images, detection_kwargs, recognition_kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recognition_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     recognition_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 62\u001b[0m box_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector\u001b[38;5;241m.\u001b[39mdetect(images\u001b[38;5;241m=\u001b[39mimages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdetection_kwargs)\n\u001b[0;32m     63\u001b[0m prediction_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognizer\u001b[38;5;241m.\u001b[39mrecognize_from_boxes(\n\u001b[0;32m     64\u001b[0m     images\u001b[38;5;241m=\u001b[39mimages, box_groups\u001b[38;5;241m=\u001b[39mbox_groups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrecognition_kwargs\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m box_groups \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     tools\u001b[38;5;241m.\u001b[39madjust_boxes(boxes\u001b[38;5;241m=\u001b[39mboxes, boxes_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m scale)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m boxes\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m boxes, scale \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(box_groups, scales)\n\u001b[0;32m     71\u001b[0m ]\n",
      "File \u001b[1;32mc:\\CodeForClass\\imgtool\\backend\\venv\\lib\\site-packages\\keras_ocr\\detection.py:777\u001b[0m, in \u001b[0;36mDetector.detect\u001b[1;34m(self, images, detection_threshold, text_threshold, link_threshold, size_threshold, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    747\u001b[0m     images: typing\u001b[38;5;241m.\u001b[39mList[typing\u001b[38;5;241m.\u001b[39mUnion[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    753\u001b[0m ):\n\u001b[0;32m    754\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Recognize the text in a set of images.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;124;03m        size_threshold: The minimum area for a word.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 777\u001b[0m     images \u001b[38;5;241m=\u001b[39m [compute_input(tools\u001b[38;5;241m.\u001b[39mread(image)) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    778\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m getBoxes(\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(images), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    780\u001b[0m         detection_threshold\u001b[38;5;241m=\u001b[39mdetection_threshold,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m         size_threshold\u001b[38;5;241m=\u001b[39msize_threshold,\n\u001b[0;32m    784\u001b[0m     )\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m boxes\n",
      "File \u001b[1;32mc:\\CodeForClass\\imgtool\\backend\\venv\\lib\\site-packages\\keras_ocr\\detection.py:777\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    747\u001b[0m     images: typing\u001b[38;5;241m.\u001b[39mList[typing\u001b[38;5;241m.\u001b[39mUnion[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    753\u001b[0m ):\n\u001b[0;32m    754\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Recognize the text in a set of images.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;124;03m        size_threshold: The minimum area for a word.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 777\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mcompute_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    778\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m getBoxes(\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(images), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    780\u001b[0m         detection_threshold\u001b[38;5;241m=\u001b[39mdetection_threshold,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m         size_threshold\u001b[38;5;241m=\u001b[39msize_threshold,\n\u001b[0;32m    784\u001b[0m     )\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m boxes\n",
      "File \u001b[1;32mc:\\CodeForClass\\imgtool\\backend\\venv\\lib\\site-packages\\keras_ocr\\detection.py:40\u001b[0m, in \u001b[0;36mcompute_input\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     37\u001b[0m mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m])\n\u001b[0;32m     38\u001b[0m variance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m---> 40\u001b[0m image \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     41\u001b[0m image \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m variance \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1500,6) (3,) (1500,6) "
     ]
    }
   ],
   "source": [
    "# im_b = Image.open(TEST_IMG)\n",
    "# b = io.BytesIO()\n",
    "# im_b.save(b, 'jpeg')\n",
    "bboxs = get_images_elem(im)\n",
    "print(bboxs)\n",
    "# display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((402, 76), (592, 105))\n"
     ]
    }
   ],
   "source": [
    "# crop an area two times larger than bbox\n",
    "bbox = bboxs[0]\n",
    "\n",
    "mask = np.zeros_like(im)\n",
    "\n",
    "MAX_HEIGHT = im.shape[0]\n",
    "MAX_WIDTH = im.shape[1]\n",
    "l = int(float(bbox['left'])); w = int(float(bbox['width'])); \n",
    "t = int(float(bbox['top'])); h = int(float(bbox['height']))\n",
    "rect = ((l, t), (l + w, t + h))\n",
    "print(rect)\n",
    "\n",
    "# cv.rectangle(mask, rect[0], rect[1], (255, 255, 255), -1)\n",
    "# display(mask)\n",
    "# cv.waitKey()\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix(angle):\n",
    "    angle_in_radians = np.deg2rad(angle)\n",
    "    c, s = np.cos(angle_in_radians), np.sin(angle_in_radians)\n",
    "    return np.array([[c, -s], [s, c]]), np.array([[c, s], [-s, c]])\n",
    "\n",
    "def preprocess(im, textbox):\n",
    "    '''\n",
    "        input: image, textbox\n",
    "        output: area near textbox, rotated\n",
    "    '''\n",
    "    image_center = (int(float(textbox['left'])), int(float(textbox['top'])))\n",
    "    br_corner = (image_center[0] + int(float(textbox['width'])), image_center[1] + int(float(textbox['height'])))\n",
    "    angle = float(textbox['angle'])\n",
    "\n",
    "    blk = np.zeros(im.shape, np.uint8)\n",
    "    cv.rectangle(blk, image_center, br_corner, (0,255,0, 0.5), -1)\n",
    "    result = cv.addWeighted(im, 1.0, blk, 0.5, 1)\n",
    "\n",
    "    rot_mat = cv.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    result = cv.warpAffine(result, rot_mat, im.shape[1::-1], flags=cv.INTER_LINEAR)\n",
    "    display(result)\n",
    "    cv.waitKey()\n",
    "    cv.destroyAllWindows()\n",
    "    return result\n",
    "\n",
    "im_r = cv.imread(TEST_IMG)\n",
    "new_im_r = preprocess(im_r, bboxs[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
